### model
model_name_or_path: /mnt/userData/geminiNfsData/pavostor/gemini/traindata/gowrg4xcai7c/393415872535859200/mistral-7B

### method
stage: dpo
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 32
lora_alpha: 64
lora_dropout: 0.05
pref_beta: 2
simpo_gamma: 1.0
pref_loss: simpo  # [sigmoid (dpo), orpo, simpo]


### ddp
ddp_timeout: 180000000
deepspeed: examples/deepspeed/ds_z3_config.json

### dataset
dataset: ultraFeedBack
template: mistral
cutoff_len: 512
max_samples: 100000000
overwrite_cache: false
preprocessing_num_workers: 8

output_dir: /mnt/userData/geminiNfsData/pavostor/gemini/traindata/gowrg4xcai7c/393415872535859200/Customization-7Bs/mistral-lora/whitebox/mistral-7B-alignment-whitebox
logging_steps: 10
save_steps: 1000
plot_loss: true
overwrite_output_dir: true
report_to: wandb
run_name: Mistral-7B-alignment-Cust-lora-whitebox # 可选

### train
per_device_train_batch_size: 32
gradient_accumulation_steps: 2
learning_rate: 5.0e-7
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.03
bf16: true

### eval
val_size: 0.08
per_device_eval_batch_size: 32
eval_strategy: steps
eval_steps: 1000
